{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "# Begin with 1211 causes from news and literature\n",
    "for country in countries:\n",
    "    tf_cause = {}\n",
    "    num_causes = 0\n",
    "    file_type = 'cause'\n",
    "    \n",
    "    # These csv files are results from running the semantic frame parsing algorithm on news and literature\n",
    "    df = pd.read_csv('results/{}_seeded_{}_cluster.csv'.format(country, file_type), delimiter=',')\n",
    "    j = 1\n",
    "    cause_thr = 0\n",
    "    for cause, cause_sim in zip(df[df['{}_cluster'.format(file_type)] == i][file_type],\n",
    "                                df[df['{}_cluster'.format(file_type)] == i]['cause_similarity']):\n",
    "        num_causes += 1\n",
    "        tokenized_doc  = word_tokenize(cause)\n",
    "        tagged_sentences = pos_tag(tokenized_doc)\n",
    "        stem_cause = \" \".join([ps.stem(w) for w in cause.split(\" \")])\n",
    "        c_pre = ''\n",
    "        for tag in tagged_sentences:\n",
    "            c = ps.stem(tag[0])\n",
    "            t = tag[1]\n",
    "            if c in stemmed_stopwords or (not c.isalpha()) or c in black_list:\n",
    "                continue\n",
    "            if t.startswith('JJ'):\n",
    "                c_pre = c\n",
    "                continue\n",
    "            if c_pre:\n",
    "                c = c_pre + ' ' + c\n",
    "                c_pre = ''\n",
    "            if t.startswith('NN') and c_pre == '':\n",
    "                c_pre = c\n",
    "            tf_cause[c] = cause_sim[0]               \n",
    "    cause_df = pd.DataFrame.from_dict(tf_cause, orient='index', columns=['count'])\n",
    "    cause_df = cause_df.reset_index()\n",
    "    causes_df = pd.concat([causes_df, cause_df], axis=0)\n",
    "\n",
    "# Expanding \n",
    "for word, count in all_words.items():\n",
    "    if count <= 1000:\n",
    "        continue\n",
    "    for cause in causes_df['cause'].unique():\n",
    "        if glove_model.wmdistance(cause.split(), word.split()) < 6:\n",
    "            tf_cause = {'cause': word, 'dist': glove_model.wmdistance(cause.split(), word.split())}\n",
    "            cause_df = pd.DataFrame.from_dict(tf_cause)\n",
    "            cause_df = cause_df.reset_index()\n",
    "            causes_df = pd.concat([causes_df, cause_df])\n",
    "\n",
    "causes_df.to_csv('expanded_cluster_causes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get p-value of granger test for each of the variables with FEWS score\n",
    "sign_ind = [i for i,v in enumerate(list(causes_df['p_value']<0.01)) if v]\n",
    "causes = [causes_df.loc[i, 'cause'] for i in sign_ind]\n",
    "X_filtered = [X[i] for i in sign_ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
